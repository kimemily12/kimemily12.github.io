<html xmlns="http://www.w3.org/1999/xhtml">

<head>
	<title>Emily Kim</title>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<link href='https://fonts.googleapis.com/css?family=Open+Sans:300,600' rel='stylesheet' type='text/css'>
	<link rel="stylesheet" type="text/css" href="style.css">

</head>

<body>
	<table border="0" cellpadding="15" cellspacing="0" width="1000">

		<div class="container">
			<div>
				<table cellspacing="0">
					<tr>
						<td width="75%">
							<h1>Emily Kim</h1>
							<p>
								I am a PhD student at Carnegie Mellon University (CMU) <a
									href="https://www.ri.cmu.edu/">Robotics Institute (RI)</a>, advised by
								Professor <a href="https://www.ri.cmu.edu/ri-faculty/jessica-k-hodgins/">Jessica
									Hodgins</a>. My research focus
								spans broadly in the field of Computer Vision and Graphics, particularly in generating
								human datasets for various tasks and vehicle detection.
								I obtained my Bachelor of Science in Joint Computer Science-Math at Harvey Mudd College.
							</p>
							<p>
								My current research focuses on human dataset for generating universal Codec Avatars and
								evaluating the avatars generated from
								cross identity subjects (with one subject driving the appearance and the other subject
								driving the expression) using the autoencoder.
								In my previous research, I have focused on RGB video-based human motion tracking for
								human action recognition (HAR) for everyday gestures and error
								diagnosis of the exercise repetitions performed during physical therapy.
								My research field also spans to the practical adversarial attaacks on vehicle detection
								from satellite images. We use both the texture (2D) and
								the mesh-based (3D) attack that attacks both the computer vision-based detection
								algorithms as well as the human perceptive system.
							</p>
							<p>
								<a href="mailto:kimemily12@gmail.com">Email</a> [kimemily12 at gmail dot com]
								<b>&#183;</b> <a href="./Resume.pdf">CV</a> [updated Mar 2024]
						</td>
						<td width="5%">
						</td>
						<td width="20%">
							<img style="max-height: 300px; max-width: 300px;" src="./portrait.jpg" border="0">
						</td>
					</tr>
				</table>
			</div>
		</div>

		<div class="container">
			<h2>News</h2>
			<ul style="list-style-type:dot">
				<ul>
					<li> (Sep 2024) Our paper on Codec Avatars dataset got accepted at NeurIPS 2024 Datasets and
						Benchmarks track for poster presentation! Check out our Github repo <a
							href="https://github.com/facebookresearch/ava-256">here</a>.
					</li>
					<li> (Mar 2024) The <a href="https://codec-avatars.github.io/cvpr24/">website</a> for CVPR Workshop
						on Codec Avatars is up!</li>
					<li> (Jan 2024) I am working with Julieta Martinez at Meta Reality Labs on the dataset and code
						release for Universal Codec Avatars. <br>
						This dataset will be presented at CVPR 2024 Codec Avatars Workshop.</li>
					<li> (Jan 2024) We presented our paper on activity recognition during the poster session at WACV
						2024.</li>
					<li> (Jan 2024) Our paper on activity recognition is released! Checkout our REMAG dataset suite <a
							href="http://humansensinglab.github.io/REMAG/">here</a>.</li>
					<li> (Oct 2023) Our paper on activity recognition has been accepted to WACV 2024! Check out our <a
							href="https://www.youtube.com/watch?v=hDlpJMyibBQ">video</a> and <a
							href="https://openaccess.thecvf.com/content/WACV2024/papers/Panev_Exploring_the_Impact_of_Rendering_Method_and_Motion_Quality_on_WACV_2024_paper.pdf">paper</a>.
					</li>
				</ul>
		</div>

		<div class="container">
			<div>
				<div>
					<h2>Publications</h2>
					<table cellspacing="15">
						<tr>
							<td width="20%">
								<img style="max-height: 250px; max-width: 250px;" src="ava-256.jpg" border="0">
							</td>
							<td>
								<p><b>
										Codec Avatar Studio: Paired Human Captures
										for Complete, Driveable, and Generalizable Avatars
									</b>
									<br>
									Julieta Martinez, <b>Emily Kim</b>, Javier Romero, Timur Bagautdinov,
									Shunsuke Saito, Shoou-I Yu, Stuart Anderson, Michael Zollh√∂fer,
									Te-Li Wang, Shaojie Bai, Shih-En Wei, Rohan Joshi, Wyatt Borsos,
									Tomas Simon, Jason Saragih, Paul Theodosis, Alexander Greene, Anjani Josyula,
									Silvio Mano Maeta, Andrew I. Jewett, Simon Venshtain, Christopher Heilman,
									Yueh-Tung Chen, Sidi Fu, Mohamed Ezzeldin A. Elshaer, Tingfang Du, Longhua Wu,
									Shen-Chi Chen, Kai Kang, Michael Wu, Youssef Emad, Steven Longay, Ashley Brewer,
									Hitesh Shah, James Booth, Taylor Koska, Kayla Haidle, Matt Andromalos,
									Joanna Hsu, Thomas Dauer, Peter Selednik, Tim Godisart, Scott Ardisson,
									Matthew Cipperly, Ben Humberston, Lon Farr, Bob Hansen, Peihong Guo,
									Dave Braun, Steven Krenn, He Wen, Lucas Evans, Natalia Fadeeva,
									Matthew Stewart, Gabriel Schwartz, Divam Gupta, Gyeongsik Moon,
									Kaiwen Guo, Yuan Dong, Yichen Xu, Takaaki Shiratori, Fabian Prada,
									Bernardo R. Pires, Bo Peng, Julia Buffalini, Autumn Trimble,
									Kevyn McPhail, Melissa Schoeller, Yaser Sheikh
									<br>
									<i>Neural Information Processing Systems (NeurIPS 2024 Datasets and Benchmarks)</i>
									<br>
									[paper], <a href="https://about.meta.com/realitylabs/codecavatars/ava256/">[Dataset
										Website]</a>, <a href="https://github.com/facebookresearch/ava-256">[code]</a>
								</p>
							</td>
						</tr>

						<tr>
							<td width="20%">
								<img style="max-height: 250px; max-width: 250px;" src="Teaser.png" border="0">
							</td>

							<td>
								<p><b> Exploring the Impact of Rendering Method and Motion Quality
										on Model Performance when Using a Multi-view Synthetic Data for Action
										Recognition </b>
									<br>
									<b>Emily Kim*</b>, Stanislav Panev*, Sai Abhishek Si Namburu, Desislava Nikolova,
									Celso de Melo, Fernando De la Torre, Jessica Hodgins
									<br>
									<i>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2024)</i>
									<br>
									<a
										href="https://openaccess.thecvf.com/content/WACV2024/papers/Panev_Exploring_the_Impact_of_Rendering_Method_and_Motion_Quality_on_WACV_2024_paper.pdf">[Open
										Access]</a>
							</td>
						</tr>
						<tr>
							<td width="30%">
								<img style="max-height: 250px; max-width: 250px;" src="AdvAttack_Teaser.png" border="0">
							</td>

							<td>
								<p><b> Texture- and Shape-based Adversarial Attacks for Vehicle Detection in Synthetic
										Overhead Imagery </b>
									<br>
									Mikael Yeghiazaryan, Sid Namburu, <b>Emily Kim</b>, Stanislav Panev, Fernando de la
									Torre,
									Jessica Hodgins, Celso de Melo
									<br>
									<i>Submitted to WACV 2025</i>
									<!-- We use a differentiable renderer to generate images and adversarially attack the vehicle detection models from satellite images in the synthetic domain.
							We use both texture (2D) and mesh-based (3D) to make it easy to implement the resulting vehicles in the real world scenario. Our goal is to attack both the 
							computer vision-based models such as RetinaNet, FasterRCNN, and YOLO as well as the human perceptive system in a practical manner, thereby constraining 
							the attack, i.e. limiting the colors of the adversarial texture to those extracted from the real-world background images, pixelating the texture, and 
							limiting the amount of displacement of the mesh vertices during the mesh attack. -->
							</td>
						</tr>
						<tr>
							<td width="30%">
								<img style="max-height: 250px; max-width: 250px;" src="popchak_teaser.png" border="0">
							</td>
							<td>
								<p><b>Sensor-Based Evaluation of Physical Therapy Exercises</b>
									<br>
									Andrew Whitford, <b>Emily Kim</b>, Eni Halilaj, Keelan Enseki, Adam Popchak, Jessica
									Hodgins
									<br>
									<i>International Conference of the IEEE Engineering in Medicine and Biology Society
										(EMBC 2021)</i>
									<br>
									<a href="https://pubmed.ncbi.nlm.nih.gov/34892839/">[paper]</a>
							</td>
						</tr>

					</table>
				</div>
			</div>
		</div>

		<!-- <div class="container">
			<div>
				<div>
					<h2>Other Projects</h2>
					<table cellspacing="15">
						<tr>

							<td>
								<p><b> Learning to Attack Vehicle Detection Models Trained on Satellite Images using
										Both the 2D Texture and 3D Mesh Deformation in the Synthetic Domain </b>
									<br>
									We use a differentiable renderer to generate images and adversarially attack the
									vehicle detection models from satellite images in the synthetic domain.
									We use both texture (2D) and mesh-based (3D) to make it easy to implement the
									resulting vehicles in the real world scenario. Our goal is to attack both the
									computer vision-based models such as RetinaNet, FasterRCNN, and YOLO as well as the
									human perceptive system in a practical manner, thereby constraining
									the attack, i.e. limiting the colors of the adversarial texture to those extracted
									from the real-world background images, pixelating the texture, and
									limiting the amount of displacement of the mesh vertices during the mesh attack.
							</td>
						</tr>

					</table>
				</div>
			</div>
		</div> -->

		<div class="container">
			<h2>Experiences</h2>
			<!-- <ul style="list-style-type:dot"> -->
			<ul>
				<li>
					2022.5 - 2022.9: Lumiere Education Research Mentor
				</li>
				<li>
					2019.9 - Now: Carnegie Mellon University Robotics Institute PhD
				</li>
				<li>
					2018.5 - 2018.8: Juniper Networks, Inc. - Project Management Intern
				</li>
				<li>
					2015.8 - 2019.5: Harvey Mudd College, BS in Computer Science-Math
				</li>
			</ul>
		</div>
	</table>

	<embed type="text/html" src="googlee7b8b4b90cf937c4.html" width="10" height="20">

</body>

</html>